---
LINK:
  - ""
  - "[[Deep Learning]]"
  - "[[Text Mining]]"
  - "[[Natural Language Processing NLP]]"
  - "[[Data Science/Neural Networks 神經網絡/Neural Networks 神經網絡]]"
File: Data Science/Deep Learning/Transformer language models.md
---

Self-attention to replace RNN:
- self-attention does not require BPTT
- optimisation is faster, so can optimise bigger language models on more data
- Bigger language models lead to improvements on almost all NLP tasks
- Train even bigger language models


























































