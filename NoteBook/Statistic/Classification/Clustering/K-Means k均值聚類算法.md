---
LINK:
  - "[[Classification]]"
  - "[[Clustering]]"
---

é¦–å…ˆå‡å®šæœ‰çµ¦å®šæ•¸é‡Kå€‹å¾…æŸ¥æ‰¾èšé¡ï¼Œç„¶å¾Œå˜—è©¦ç‚ºé€™äº›èšé¡é¸æ“‡ä¸­å¿ƒï¼Œä½¿å¾—ä¸­å¿ƒèˆ‡èšé¡æˆå“¡ä¹‹é–“çš„è·é›¢è¼ƒå°

å¯†åˆ‡ç›¸é—œçš„åŸºæ–¼æ¨¡å‹çš„èšé¡ç®—æ³•ä¹Ÿè¦æ±‚æˆ‘å€‘æŒ‡å®šèšé¡çš„æ•¸é‡ï¼Œç„¶å¾Œå‡è¨­ä¸€å€‹åº•å±¤æ¦‚ç‡æ¨¡å‹ï¼Œä¸¦ç”¨å¼·å¤§çš„EMç®—æ³•ä»¥æœ€å¤§ä¼¼ç„¶çš„æ–¹æ³•å°æˆ‘é€²è¡Œæ“¬åˆ


begin with a collection of data points $\mathbf{x}_j \in \mathbb{R}^d$
$$
\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n
$$
and seek to separate them into $k$ clusters, $C_{1},C_{2},\dots,C_{k}$ where the points in each cluster are close to each other æ¯å€‹ç°‡ä¸­çš„é»å½¼æ­¤æ¥è¿‘ï¼Œä»¥Euclidean distance æ¸¬é‡è·é›¢$$
d(\mathbf{x}_j, \mathbf{x}_k) = \|\mathbf{x}_j - \mathbf{x}_k\|_2
$$ where $$
\|\mathbf{u}\|_2 = \sqrt{\sum_{j=1}^{d} u_j^2} = \sqrt{u_1^2 + \cdots + u_d^2}.
$$
Arrange the points $x_{1},x_{2},\dots,x_{n}$ into $k$ groups $C_{1},C_{2},\dots,C_{k}$ (with k $\leq$ n) ---- called clusters so that a certain total within -cluster sum of squares is minimisedæŸå€‹èšé¡å…§ç¸½å’Œå¹³æ–¹å’Œæœ€å°åŒ–:  $$
\min_{C} \sum_{j=1}^{k} \sum_{\mathbf{x} \in C_j} \|\mathbf{x} - \boldsymbol{\mu}_j\|_2^2.
$$
é€™è£¡å–æœ€å°å€¼ï¼Œè¡¨ç¤ºæ‰€æœ‰å¯èƒ½çš„é»åˆ°ç°‡çš„åˆ†é…æƒ…æ³ï¼Œ$|C_{j}|$ is the number of points in the j-th cluster $\mu_j = \frac{1}{|C_j|} \sum_{\mathbf{x} \in C_j} \mathbf{x}$
is the mean position of the points in the j-th cluster and is called the clusterâ€™s centre æ˜¯ç¬¬jå€‹ç°‡é‡é»çš„å¹³å‡ä½ç½®ï¼Œæˆç‚ºè©²ç°‡çš„ä¸­å¿ƒ

![](PICTURE/K-Means%20k%E5%9D%87%E5%80%BC%E8%81%9A%E9%A1%9E%E7%AE%97%E6%B3%95/8d109f0f413f2ab295d76e2ade32323b_MD5.jpeg)

One positions the clusters through an iterative process in which one alternately é€šéè¿­ä»£éç¨‹ä¾†å®šä½ç°‡ éç¨‹ä¸­äº¤æ›¿
- assigns points to the nearest centre å°‡é»åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
- moves each centre to the middle of its cloud of points å°‡æ¯å€‹ä¸­å¿ƒç§»å‹•åˆ°å…¶é»é›²çš„ä¸­é–“
ä¸­å¿ƒä»¥è±å½¢è¡¨ç¤ºï¼Œé›†ç¾¤æˆå“¡èº«ä»½ä»¥é¡è‰²è¡¨ç¤º


éç¨‹ï¼š
- Choose initial positions for the centers $\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k$ é¸æ“‡ä¸­å¿ƒçš„åˆå§‹ä½ç½®
- å°æ–¼æ¯å€‹é» $x_{i}$ è¨ˆç®— $x_{i}$ åˆ° $k$ å€‹ä¸­å¿ƒçš„Euclidean distancesï¼Œ ä¸¦å°‡xåˆ†é…åˆ°ä¸­å¿ƒé™„è¿‘çš„èšé¡
- å°‡æ‰€æœ‰åº—åˆ†é…åˆ°èšé¡å¾Œï¼Œé€šéèšé¡æˆå“¡å–å¹³å‡å€¼ä¾†è¨ˆç®—æ–°çš„ä¸­å¿ƒ $\boldsymbol{\mu}'_{j}$  $$
\mu'_j = \frac{1}{|C_j|} \sum_{\mathbf{x} \in C_j} \mathbf{x}.
$$
- å¦‚æœä¸­å¿ƒæ²’æœ‰ç§»å‹• $\boldsymbol{\mu}_{j} = \boldsymbol{\mu}'_{j}$ å‰‡åœæ­¢ï¼Œå¦å‰‡è¿”å›æ­¥é©Ÿ2ï¼Œä¸¦æ ¹æ“šæ–°ä¸­å¿ƒçµ„è£æ–°ç°‡


> [!check]+ Choosing the initial centres é¸æ“‡åˆå§‹ä¸­å¿ƒ
There are lots of ways to do this. Common approaches include: 
â€¢ Choose $k$ points uniformly and at random inside a box defined by the ranges of the data. This risks choosing centres that are far from any of the $x_j$ . åœ¨æ•¸æ“šç¯„åœå®šç¾©çš„æ¡†å…§å‡å‹»éš¨æ©Ÿåœ°é¸æ“‡ $k$ å€‹é» é€™å¯èƒ½æœƒé¸æ“‡é é›¢ä»»ä½•xçš„ä¸­å¿ƒ
â€¢ Forgyâ€™s method: choose $k$ of the data points themselves. One risks getting all the centres close together or, at the other extreme, selecting outliers that arenâ€™t especially near any other points. é¸æ“‡kå€‹æ•¸æ“šé»æœ¬èº« é€™æ¨£åšçš„é¢¨éšªæ˜¯è®“æ‰€æœ‰ä¸­å¿ƒéƒ½é çš„å¾ˆè¿‘
â€¢ Random partitionéš¨æ©Ÿåˆ†å€: assign the points to clusters uniformly at random, so each cluster starts with around $n/k$ points, then use the cluster means as initial centres. 
â€¢ Choose the most centrally-located point as $Î¼_1$, then work through the data and take $Î¼_2$ to be the next point that is at least a distance $T$ away . . . . This, of course, depends on $T$ and the ordering of the data.
â€¢ Maximin: choose $Î¼_1$ arbitrarily, then choose subsequent centres to be the data point with largest minimum-distance to all the existing centres

è©²ç®—æ³•æœ€çµ‚æœƒåœæ­¢ï¼Œä½†å¾—åˆ°çš„èšé¡çµæœå–æ±ºæ–¼åˆå§‹ä¸­å¿ƒ(the clustering one gets depends on the initial centres)ã€‚ å› æ­¤æœ€å¥½ä½¿ç”¨ä¸åŒçš„åˆå§‹ä¸­å¿ƒé‹ç®—è©²ç®—æ³•å¹¾æ¬¡ï¼Œç„¶å¾Œé¸æ“‡å¹³æ–¹å’Œæœ€å°çš„çµæœ


å¯èƒ½æœƒç™¼ç”Ÿé›†ç¾¤æœ€çµ‚ç‚ºç©ºçš„æƒ…æ³

### Choosing the number of clusters:
#### make an elbow plot è‚˜éƒ¨åœ–

For each of a range of values of k, run the k-means algorithm repeatedly, with dierent random starts, and keep track of the sum-of-squares achieved. Typically this decreases rapidly for a while, then more slowly. Choose the value of k where the break in slope occurså°æ–¼kçš„æ¯å€‹å€¼ç¯„åœï¼Œé‡è¤‡é‹è¡Œkå‡å€¼ç®—æ³•ï¼Œä½¿ç”¨ä¸åŒçš„éš¨æ©Ÿèµ·é»ï¼Œä¸¦è·Ÿè¹¤æ‰€å¯¦ç¾çš„å¹³æ–¹å’Œã€‚é€šå¸¸ï¼Œå¹³æ–¹å’Œæœƒåœ¨ä¸€æ®µæ™‚é–“å…§è¿…é€Ÿä¸‹é™ï¼Œç„¶å¾Œæœƒè®Šæ…¢ã€‚é¸æ“‡æ´©éœ²å‡ºç¾çªè®Šçš„kå€¼
![](PICTURE/K-Means%20k%E5%9D%87%E5%80%BC%E8%81%9A%E9%A1%9E%E7%AE%97%E6%B3%95/709830c4b9fdc7cb1f4375bf17cf1eed_MD5.jpeg)




### problemï¼š
K-means seeks to minimize the distance from the centres to the points in the cluster, which implies that it can behave badly when the natural clusters arenâ€™t balls  å¯»æ±‚æœ€å°åŒ–ä»ä¸­å¿ƒåˆ°èšç±»ä¸­ç‚¹çš„è·ç¦» å¯»æ±‚æœ€å°åŒ–ä»ä¸­å¿ƒåˆ°èšç±»ä¸­ç‚¹çš„è·ç¦»

å¯ä»¥é€šéç¹ªè£½ elbow plotä¾†æŸ¥çœ‹å¤šå°‘å€‹èšé¡çš„æ™‚å€™æœ€åˆé©

![](PICTURE/K-Means%20k%E5%9D%87%E5%80%BC%E8%81%9A%E9%A1%9E%E7%AE%97%E6%B3%95/2493033c0336162845e7ffe64c165223_MD5.jpeg)

å¦‚åœ– å¯ä»¥çœ‹åˆ° k=3çš„æ™‚å€™ æœ€åˆé©

### ADV and DIS 
Some strengths:
- Simple to implement æ˜“æ–¼å¯¦ç¾
- Scales to large datasets æ“´å±•åˆ°å¤§å‹æ•¸æ“šé›†
- Always converges ç¸½æ˜¯æ”¶æ–‚
Some limitations:
- k must be pre-specifiedå¿…é ˆé å…ˆæŒ‡å®š â†’ can try different values, but must be done manually  å¯ä»¥å˜—è©¦ä¸åŒå€¼ ä½†å¿…é ˆæ‰‹å‹•å®Œæˆ 
- Results depend on initial configuration çµæœå–æ±ºæ–¼åˆå§‹é…ç½® â†’ try various starting choices (or even methods) 
- Difficulty clustering data on varying sizes and densities without generalisation é›£ä»¥å°ä¸åŒå¤§å°å’Œå¯†åº¦çš„æ•¸æ“šé€²è¡Œèšé¡ï¼Œä¸”ç„¡æ³•é€²è¡Œæ³›åŒ–
	â†’ see kernel-k-means and model-based clustering 
- Difficulty clustering outliers é›£ä»¥èšé¡çš„ç•°å¸¸å€¼ â†’ consider removing them first 
- Difficulty scaling with number of dimensions (curse of dimensionality) â†’ reduce dimensionality first by applying PCA (spectral clustering)



## kernel k-means
â€¢ Main idea: transform the data into a different, higher-dimensional space where k-means can more easily separate natural clusters.  å°‡æ•¸æ“šè½‰æ›åˆ°ä¸åŒçš„ã€æ›´é«˜ç¶­çš„ç©ºé–“ä¸­åœ¨è©²ç©ºé–“ä¸­ l-meanå¯ä»¥æ›´å®¹æ˜“åœ°åˆ†é›¢è‡ªç„¶èšé¡
â€¢ Here the map is Ï† :  R2 â†’ R3 with Ï†(x1, x2) = (x21, x1x2, x22).
â€¢ In the plot at right below, points are coloured according to the clusters assigned by k-means with k = 2 in the transformed space

![](PICTURE/K-Means%20k%E5%9D%87%E5%80%BC%E8%81%9A%E9%A1%9E%E7%AE%97%E6%B3%95/f2439dee1198ebd8978a80b24fbc2b79_MD5.jpeg)

åŸå§‹ç©ºé–“ (å¹³é¢)ï¼š ç¶ è‰²åœ¨ä¸­é–“ï¼Œæ©˜è‰²é»åœ¨å¤–åœåŸç’°å½¢ é€™èªªæ˜è³‡æ–™åœ¨åŸå§‹çš„2Dç©ºé–“ä¸­ä¸æ˜¯ç·šæ€§å¯åˆ†çš„(linearly divisible)ï¼›  å‚³çµ±K-Meanä½¿ç”¨Euclidean distance ç•«å‡ºåœ“å¿ƒé»

å°dataåšéç·šæ€§æ˜ å°„ non-linear mapping 
ä½¿ç”¨æ ¸å‡½æ•¸å®šç¾©çš„é«˜ç¶­è·é›¢

kernel æ˜¯ ç”¨æ–¼å°‡æ•¸æ“šè½‰æ›åˆ°é«˜ç¶­ç©ºé–“ç„¶å¾Œå†é€²è¡Œk-mean 

å¸¸è¦‹çš„kernelï¼š

| Kernel åç¨±        | æ•¸å­¸å½¢å¼                                                     | é©åˆçš„æƒ…å¢ƒ               |                 |                |
| ---------------- | -------------------------------------------------------- | ------------------- | --------------- | -------------- |
| **Linear**       | K(x,y)=xâŠ¤yK(x, y) = x^\top y                             | å’Œæ™®é€š K-means ä¸€æ¨£ï¼Œç·šæ€§å¯åˆ† |                 |                |
| **Polynomial**   | K(x,y)=(xâŠ¤y+c)dK(x, y) = (x^\top y + c)^d                | æœ‰å½æ›²é‚Šç•Œçš„åˆ†ç¾¤            |                 |                |
| **RBF/Gaussian** | K(x,y)=expâ¡(âˆ’âˆ¥xâˆ’yâˆ¥22Ïƒ2)K(x, y) = \exp(-\frac{\\          | x - y\\             | ^2}{2\sigma^2}) | åœ“å½¢ã€ç’°å½¢ã€ä»»æ„å½¢ç‹€ç¾¤é«”   |
| **Sigmoid**      | K(x,y)=tanhâ¡(Î±xâŠ¤y+c)K(x, y) = \tanh(\alpha x^\top y + c) | é¡ä¼¼ç¥ç¶“ç¶²çµ¡çš„æ¿€æ´»æ•ˆæœ         |                 |                |
| **Laplacian**    | K(x,y)=expâ¡(âˆ’âˆ¥xâˆ’yâˆ¥Ïƒ)K(x, y) = \exp(-\frac{\\             | x - y\\             | }{\sigma})      | å° outlier æ¯”è¼ƒå¥å£¯ |


1. å°‡é»éš¨æ©Ÿå‡å‹»çš„åˆ†é…åˆ°èšé¡ä¸­ã€‚
2. å°æ–¼æ¯å€‹é»xå’Œæ¯å€‹èšé¡Cè¨ˆç®—è·é›¢
3. é€šéå°‡æ¯å€‹é»æ”¾å…¥ä¸­å¿ƒæœ€è¿‘çš„èšé¡ä¸­ä¾†æ›´æ–°èšé¡åˆ†é…
4. å¦‚æœä¸Šä¸€æ­¥ä¸­èšé¡åˆ†é…æ²’æœ‰è®ŠåŒ–ï¼Œå‰‡åœæ­¢ï¼Œå¦å‰‡è¿”å›ç¬¬äºŒæ­¥ä¸¦æ ¹æ“šæ–°çš„åˆ†é¡åˆ†é…è¨ˆç®—æ–°çš„ä¸­å¿ƒ

Kernel Function æ˜¯å°ç¨±çš„



![](PICTURE/K-Means%20k%E5%9D%87%E5%80%BC%E8%81%9A%E9%A1%9E%E7%AE%97%E6%B3%95/e0f33306e742519c73afcd900b70a6f6_MD5.jpeg)
















